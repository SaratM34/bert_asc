05/26/2019 21:48:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../pt_model/rest_pt/vocab.txt
05/26/2019 21:48:12 - INFO - __main__ -   ***** Running training *****
05/26/2019 21:48:12 - INFO - __main__ -     Num examples = 3452
05/26/2019 21:48:12 - INFO - __main__ -     Batch size = 32
05/26/2019 21:48:12 - INFO - __main__ -     Num steps = 428
05/26/2019 21:48:12 - INFO - __main__ -   ***** Running validations *****
05/26/2019 21:48:12 - INFO - __main__ -     Num orig examples = 150
05/26/2019 21:48:12 - INFO - __main__ -     Num split examples = 150
05/26/2019 21:48:12 - INFO - __main__ -     Batch size = 32
05/26/2019 21:48:12 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ../pt_model/rest_pt/
05/26/2019 21:48:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/26/2019 21:48:15 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/26/2019 21:48:15 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
05/26/2019 21:49:33 - INFO - __main__ -   validation loss: 0.733545
05/26/2019 21:50:48 - INFO - __main__ -   validation loss: 0.654001
05/26/2019 21:52:03 - INFO - __main__ -   validation loss: 0.538365
05/26/2019 21:53:16 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
05/26/2019 21:53:17 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
05/26/2019 21:53:17 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
05/26/2019 21:53:19 - INFO - __main__ -   validation loss: 0.557051
